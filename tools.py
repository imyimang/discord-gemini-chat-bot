import requests
import urllib.parse
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import re
import os
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from datetime import datetime
import isodate

load_dotenv()

APP_ID = os.getenv('APP_ID')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
SEARCH_ENGINE_ID = os.getenv('SEARCH_ENGINE_ID')
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
url = "https://www.googleapis.com/customsearch/v1"

def google(s):
    params = {
        "key": GOOGLE_API_KEY,
        "cx": SEARCH_ENGINE_ID,
        "q": s,
    }

    response = requests.get(url, params=params)
    results = response.json()

    output = []

    for item in results.get("items", []):
        title = "æ¨™é¡Œï¼š " + item["title"]
        about = "ç°¡ä»‹ï¼š " + item.get("snippet", "")
        link  = "é€£çµï¼š " + item.get("link")
        output.append(f"{title}\n{about}\n{link}\n-----\n")

    return "æœå°‹çµæœ: " + "\n".join(output) if output else "æ‰¾ä¸åˆ°ä»»ä½•çµæœ"


def wolframalpha(query: str, query_or : str):
    encoded_query = urllib.parse.quote(query)
    url = f"https://api.wolframalpha.com/v2/query?appid={APP_ID}&input={encoded_query}"

    response = requests.get(url)

    if response.status_code == 200:
        root = ET.fromstring(response.content)
        results = []

        for pod in root.findall(".//pod"):
            title = pod.attrib.get("title")
            texts = []
            for subpod in pod.findall("subpod"):
                plaintext = subpod.find("plaintext")
                if plaintext is not None and plaintext.text:
                    texts.append(plaintext.text.strip())

            if texts:
                # æŠŠ title å’Œå…§å®¹æ•´ç†æˆä¸€æ®µæ®µæ–‡å­—
                result_block = f"ã€{title}ã€‘\n" + "\n".join(texts)
                results.append(result_block)

        if results:
            return "\n\n".join(results)
        else:
            return "wolframalphaæœå°‹æ²’æœ‰çµæœï¼Œgoogleæœå°‹çµæœå¦‚ä¸‹" + google(query_or)
    else:
        return f"WolframAlpha å›å‚³éŒ¯èª¤ï¼š{response.status_code}"

def get_news(category):
    result = ""
    category_url = f"https://www.nownews.com/cat/{category}/"

    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    res = requests.get(category_url, headers=headers)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    left_col = soup.find("div", class_="leftCol")
    links = left_col.find_all("a", class_="trace-click")

    visited = set()
    count = 0

    for link in links:
        if count >= 5:
            break

        href = link.get("href")
        if href and href.startswith("https://www.nownews.com/news/") and href not in visited:
            visited.add(href)

            try:
                article_res = requests.get(href, headers=headers)
                article_res.encoding = "utf-8"
                article_soup = BeautifulSoup(article_res.text, "html.parser")

                title_tag = article_soup.find("h1", class_="article-title")
                title = title_tag.text.strip() if title_tag else "æ¨™é¡Œæœªæ‰¾åˆ°"

                content_div = article_soup.find("div", id="articleContent")
                if content_div:
                    for ad in content_div.find_all(class_=["ad-blk", "ad-blk1"]):
                        ad.decompose()

                    for br in content_div.find_all("br"):
                        br.replace_with("\n")

                    text = content_div.get_text(strip=True, separator="\n")

                    # ç§»é™¤ â–² é–‹é ­çš„æ•´è¡Œ
                    text = re.sub(r'^â–².*(?:\n|$)', '', text, flags=re.MULTILINE)
                    text = "\n".join([line for line in text.split("\n") if line.strip()])
                else:
                    text = "æ‰¾ä¸åˆ°æ–‡ç« å…§å®¹"

                # ç´¯åŠ åˆ°çµæœå­—ä¸²
                result += f"ğŸ“° æ¨™é¡Œï¼š{title}\nğŸ“„ å…§æ–‡ï¼š\n{text}\n\n{'='*10}\n\n"
                count += 1

            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è™•ç† {href}ï¼š{e}")

    return result


async def youtube_search(query, max_results=5, language="en", duration="medium"):
    try:
        # åˆå§‹åŒ– YouTube API å®¢æˆ¶ç«¯
        youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)

        # è¨­ç½®æœå°‹åƒæ•¸
        search_params = {
            "part": "id,snippet",
            "q": query,
            "type": "video",
            "maxResults": min(max_results, 50),  # API é™åˆ¶æœ€å¤š 50 æ¢
            "relevanceLanguage": language,
            "order": "relevance"  # å¯æ”¹ç‚º "viewCount" æˆ– "date"
        }

        # ç¯©é¸å½±ç‰‡æ™‚é•·
        if duration in ["short", "medium", "long"]:
            search_params["videoDuration"] = duration

        # åŸ·è¡Œæœå°‹è«‹æ±‚
        request = youtube.search().list(**search_params)
        response = request.execute()

        # æå–å½±ç‰‡ ID æ¸…å–®
        video_ids = [item["id"]["videoId"] for item in response.get("items", [])]

        if not video_ids:
            return f"æ‰¾ä¸åˆ°èˆ‡ '{query}' ç›¸é—œçš„å½±ç‰‡ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµè©ã€‚"

        # æŸ¥è©¢å½±ç‰‡è©³ç´°è³‡è¨Šï¼ˆåŒ…æ‹¬æ™‚é•·ï¼‰
        details_request = youtube.videos().list(
            part="snippet,contentDetails",
            id=",".join(video_ids)
        )
        details_response = details_request.execute()

        # æ ¼å¼åŒ–çµæœ
        results = []
        for item in details_response.get("items", []):
            snippet = item["snippet"]
            content_details = item["contentDetails"]
            # è§£æå½±ç‰‡æ™‚é•·ï¼ˆISO 8601 æ ¼å¼ï¼‰
            duration = isodate.parse_duration(content_details["duration"])
            duration_str = f"{duration.seconds // 3600}h {duration.seconds % 3600 // 60}m" if duration.seconds >= 3600 else f"{duration.seconds // 60}m {duration.seconds % 60}s"
            # è§£æç™¼å¸ƒæ—¥æœŸ
            published_at = datetime.strptime(snippet["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d")
            # æ§‹å»ºçµæœæ¢ç›®
            result = (
                f"[{snippet['title']}](https://www.youtube.com/watch?v={item['id']}) - "
                f"ç”± {snippet['channelTitle']} ç™¼å¸ƒï¼Œæ™‚é•· {duration_str}ï¼Œ{published_at}"
            )
            results.append(result)

        # è¿”å›æ ¼å¼åŒ–çš„æ¸…å–®
        return f"\nä»¥ä¸‹æ˜¯ Youtubeä¸Š '{query}' çš„æœå°‹çµæœï¼š\n" + "\n".join(f"{i+1}. {result}" for i, result in enumerate(results))

    except HttpError as e:
        return f"YouTube API è«‹æ±‚å¤±æ•—ï¼š{str(e)}"
    except Exception as e:
        return f"æœå°‹å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"